# -*- coding: utf-8 -*-
"""Untitled22.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UafPPBmPKKsiB5xQHGuPlQVeO2h7RYV7
"""

# Importar librer√≠as
import pandas as pd
import numpy as np
import pickle

# ==============================================================================
# FASE 1: COMPRENSI√ìN DEL NEGOCIO (Par√°metros de Velocidad)
# ==============================================================================
print("--- FASE 1: DEFINICI√ìN DE OBJETIVOS (VELOCIDAD ULTRA-R√ÅPIDA) ---")

# Par√°metros √ÅGILES para M√°xima Velocidad
MAX_LEN = 10
RNN_UNITS = 16   # Reducci√≥n de 32 a 16
EMBEDDING_DIM = 32 # Reducci√≥n de 64 a 32
TEST_SIZE = 0.2
EPOCHS = 5       # Entrenamiento muy corto
BATCH_SIZE = 128 # Aumento para menos pasos por √©poca

print(f"RNN Units: {RNN_UNITS}, Embedding Dim: {EMBEDDING_DIM}, Epochs: {EPOCHS}")
print("-" * 60)

# ==============================================================================
# FASE 2: DATOS (Carga y Control de Calidad)
# ==============================================================================
print("--- FASE 2: CARGA Y CONTROL DE CALIDAD DE LOS DATOS ---")

# Carga de Datos
try:
    df = pd.read_csv('data.csv')
except FileNotFoundError:
    print("‚ùå Error: Aseg√∫rate de que 'data.csv' est√© cargado en tu entorno Colab.")
    raise

# Control de Calidad
df.dropna(inplace=True)
print(f"N√∫mero de pares de frases despu√©s de la limpieza: {len(df)}")

# Preparaci√≥n de listas de frases y etiqueta de fin
input_texts = df['spanish'].astype(str).tolist()
target_texts = df['english'].astype(str).tolist()
target_texts_output = [text + ' endofseq' for text in target_texts]

print("Etiquetas 'endofseq' a√±adidas.")
print("-" * 60)

# üíæ GUARDAR DATOS Y PAR√ÅMETROS
with open('processed_data.pkl', 'wb') as f:
    pickle.dump({
        'input_texts': input_texts,
        'target_texts_output': target_texts_output,
        'MAX_LEN': MAX_LEN, 'TEST_SIZE': TEST_SIZE,
        'RNN_UNITS': RNN_UNITS, 'EMBEDDING_DIM': EMBEDDING_DIM,
        'EPOCHS': EPOCHS, 'BATCH_SIZE': BATCH_SIZE
    }, f)
print("Datos y par√°metros guardados en 'processed_data.pkl'.")

# Importar librer√≠as
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
import pickle

# Cargar datos y par√°metros
try:
    with open('processed_data.pkl', 'rb') as f:
        data = pickle.load(f)
    input_texts = data['input_texts']
    target_texts_output = data['target_texts_output']
    MAX_LEN = data['MAX_LEN']
    TEST_SIZE = data['TEST_SIZE']
except FileNotFoundError:
    print("‚ùå Error: Ejecuta el Bloque 1 primero.")
    raise

# ==============================================================================
# FASE 3: PREPARACI√ìN DE LOS DATOS (Conversi√≥n y Alineaci√≥n)
# ==============================================================================
print("--- FASE 3: PREPARACI√ìN DE LOS DATOS ---")

# Aspecto: Conversi√≥n Num√©rica (Tokenizadores)
spa_tokenizer = Tokenizer()
spa_tokenizer.fit_on_texts(input_texts)
input_sequences = spa_tokenizer.texts_to_sequences(input_texts)
spa_vocab_size = len(spa_tokenizer.word_index) + 1

eng_tokenizer = Tokenizer(filters='')
eng_tokenizer.fit_on_texts(target_texts_output)
target_sequences = eng_tokenizer.texts_to_sequences(target_texts_output)
eng_vocab_size = len(eng_tokenizer.word_index) + 1

# Aspecto: Secuenciaci√≥n y Alineaci√≥n (Padding)
encoder_input_data = pad_sequences(input_sequences, maxlen=MAX_LEN, padding='post')
decoder_target_data = pad_sequences(target_sequences, maxlen=MAX_LEN, padding='post')
decoder_target_data = np.expand_dims(decoder_target_data, -1)

# Aspecto: Divisi√≥n Final (Train/Test Split)
X_train, X_test, y_train, y_test = train_test_split(
    encoder_input_data, decoder_target_data, test_size=TEST_SIZE, random_state=42
)
print(f"Datos de Entrenamiento (X_train): {X_train.shape}, Test (X_test): {X_test.shape}")
print("-" * 60)

# üíæ GUARDAR DATOS PROCESADOS PARA EL SIGUIENTE BLOQUE
with open('processed_data_f3.pkl', 'wb') as f:
    pickle.dump({
        'X_train': X_train, 'X_test': X_test, 'y_train': y_train, 'y_test': y_test,
        'spa_vocab_size': spa_vocab_size, 'eng_vocab_size': eng_vocab_size,
        'spa_tokenizer': spa_tokenizer, 'eng_tokenizer': eng_tokenizer,
        'MAX_LEN': MAX_LEN, 'RNN_UNITS': data['RNN_UNITS'],
        'EMBEDDING_DIM': data['EMBEDDING_DIM'], 'EPOCHS': data['EPOCHS'],
        'BATCH_SIZE': data['BATCH_SIZE']
    }, f)
print("Datos tokenizados y divididos guardados en 'processed_data_f3.pkl'.")

# Importar librer√≠as
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense
import pickle

# Cargar datos y par√°metros
try:
    with open('processed_data_f3.pkl', 'rb') as f:
        data_f3 = pickle.load(f)

    X_train = data_f3['X_train']
    y_train = data_f3['y_train']
    X_test = data_f3['X_test']
    y_test = data_f3['y_test']

    spa_vocab_size = data_f3['spa_vocab_size']
    eng_vocab_size = data_f3['eng_vocab_size']
    MAX_LEN = data_f3['MAX_LEN']
    RNN_UNITS = data_f3['RNN_UNITS']
    EMBEDDING_DIM = data_f3['EMBEDDING_DIM']
    EPOCHS = data_f3['EPOCHS']
    BATCH_SIZE = data_f3['BATCH_SIZE']

except FileNotFoundError:
    print("‚ùå Error: Ejecuta el Bloque 2 primero.")
    raise

# ==============================================================================
# FASE 4: MODELADO Y ENTRENAMIENTO
# ==============================================================================
print("--- FASE 4: MODELADO Y ENTRENAMIENTO R√ÅPIDO ---")

# Aspecto: Construcci√≥n del Modelo
model = Sequential([
    Embedding(spa_vocab_size, EMBEDDING_DIM, input_length=MAX_LEN, mask_zero=True),
    SimpleRNN(RNN_UNITS, return_sequences=True),
    Dense(eng_vocab_size, activation='softmax')
])

# Aspecto: Compilaci√≥n del Modelo
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

print("üöÄ Arquitectura del Modelo:")
model.summary()
print("-" * 30)


# Aspecto: Entrenamiento del Modelo (Ejecuci√≥n)
print(f"Iniciando **ENTRENAMIENTO** con {EPOCHS} √©pocas y BATCH_SIZE={BATCH_SIZE}...")
history = model.fit(
    X_train,
    y_train,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    validation_data=(X_test, y_test),
    verbose=1
)

print("-" * 60)
print("‚úÖ Entrenamiento completado. El historial se pasa a la Fase 5.")

# üíæ GUARDAR MODELO Y EL HISTORIAL
model.save('model_trained_fast.keras')
with open('training_history.pkl', 'wb') as f:
    pickle.dump(history.history, f)
print("Modelo entrenado y el historial de entrenamiento guardados.")

# Importar librer√≠as
import pickle
import matplotlib.pyplot as plt

# Cargar el historial de entrenamiento
try:
    with open('training_history.pkl', 'rb') as f:
        history_data = pickle.load(f)

except FileNotFoundError:
    print("‚ùå Error: Ejecuta el Bloque 3 primero.")
    raise

# ==============================================================================
# FASE 5: M√âTRICAS (Evaluaci√≥n y Visualizaci√≥n)
# ==============================================================================
print("--- FASE 5: M√âTRICAS ---")

# Aspecto: Resumen de Resultados
final_val_loss = history_data['val_loss'][-1]
final_val_accuracy = history_data['val_accuracy'][-1]

print("üìä RESUMEN DE M√âTRICAS (√öltima √âpoca):")
print(f"P√©rdida de Validaci√≥n (Loss): {final_val_loss:.4f}")
print(f"Precisi√≥n de Validaci√≥n (Accuracy): {final_val_accuracy:.4f} ({final_val_accuracy * 100:.2f}%)")
print("Nota: Los valores son bajos debido a la estrategia de entrenamiento ultra-r√°pido.")


# Aspecto: Visualizaci√≥n de Matrices (Gr√°ficos)
plt.figure(figsize=(12, 5))

# Gr√°fico de P√©rdida (Loss)
plt.subplot(1, 2, 1)
plt.plot(history_data['loss'], label='P√©rdida (Train)')
plt.plot(history_data['val_loss'], label='P√©rdida (Validation)')
plt.title('P√©rdida del Modelo por √âpoca')
plt.ylabel('P√©rdida (Loss)')
plt.xlabel('√âpoca')
plt.legend()

# Gr√°fico de Precisi√≥n (Accuracy)
plt.subplot(1, 2, 2)
plt.plot(history_data['accuracy'], label='Precisi√≥n (Train)')
plt.plot(history_data['val_accuracy'], label='Precisi√≥n (Validation)')
plt.title('Precisi√≥n del Modelo por √âpoca')
plt.ylabel('Precisi√≥n')
plt.xlabel('√âpoca')
plt.legend()

plt.show()

print("-" * 60)
print("‚úÖ An√°lisis de m√©tricas completado.")

# Importar librer√≠as
import pickle
from tensorflow.keras.models import load_model

# Cargar objetos necesarios
try:
    with open('processed_data_f3.pkl', 'rb') as f:
        data_f3 = pickle.load(f)
    spa_tokenizer = data_f3['spa_tokenizer']
    eng_tokenizer = data_f3['eng_tokenizer']

    # Verificar la existencia del modelo entrenado
    model = load_model('model_trained_fast.keras')

except FileNotFoundError:
    print("‚ùå Error: Aseg√∫rate de ejecutar el Bloque 3 primero.")
    raise

# ==============================================================================
# FASE 6: DESPLIEGUE (SERIALIZACI√ìN DE ACTIVOS)
# ==============================================================================
print("--- FASE 6: DESPLIEGUE (SERIALIZACI√ìN DE ACTIVOS) ---")

# Aspecto: Serializaci√≥n del Modelo
print("üíæ Modelo entrenado 'model_trained_fast.keras' listo para el despliegue.")

# Aspecto: Serializaci√≥n de los Tokenizadores
# Guardar el tokenizador de espa√±ol
with open('spa_tokenizer_fast.pickle', 'wb') as handle:
    pickle.dump(spa_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
print("üíæ Tokenizador de Espa√±ol (spa_tokenizer_fast.pickle) guardado.")

# Guardar el tokenizador de ingl√©s
with open('eng_tokenizer_fast.pickle', 'wb') as handle:
    pickle.dump(eng_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
print("üíæ Tokenizador de Ingl√©s (eng_tokenizer_fast.pickle) guardado.")

print("Todos los activos necesarios han sido serializados.")
print("-" * 60)